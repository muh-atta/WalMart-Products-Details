name: Run Walmart Scraper

permissions:
  contents: write

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 1 *'  # Once a year on Jan 1st at 00:00

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libx11-dev \
            libx11-xcb1 \
            libxcomposite1 \
            libxcursor1 \
            libxdamage1 \
            libxi6 \
            libxtst6 \
            libnss3 \
            libatk-bridge2.0-0 \
            libgtk-3-0 \
            libdrm2 \
            libgbm1 \
            libasound2t64

      - name: Install Node.js dependencies
        run: npm install

      - name: Install Chromium for Puppeteer
        run: npx puppeteer browsers install chrome

      - name: Run scraper script
        run: node WalmartTrigerData.js

      - name: Commit and Push Scraped Data
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config pull.rebase false  # prevent rebase errors

          git add walmart-data.csv walmart-data.json
          git commit -m "Auto update scraped data - $(date)" || echo "No changes to commit"
          git pull origin main || echo "Nothing to pull"
          git push origin HEAD:main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
